# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Boosting for Regression using Any R Model
#' 
#' Implements Algorithm 8.2 for gradient boosting with regression trees
#' or any other base learner compatible with the Model R6 class interface.
#' This implementation uses the negative gradient of squared error loss.
#' 
#' @param model_creator An R function that returns a new Model object with
#'   fit() and predict() methods
#' @param X Feature matrix (n x p)
#' @param y Response vector (length n)
#' @param B Number of boosting iterations
#' @param model_args Named list of additional arguments to pass to model$fit()
#' @param eta Shrinkage parameter (learning rate), default 0.1
#' @param verbose Whether to print progress (default: TRUE)
#' 
#' @return A list containing:
#' \itemize{
#'   \item models: List of B fitted model objects
#'   \item f_hat: Final predictions on training data
#'   \item y: Original response vector (needed for variable importance)
#'   \item residuals_history: Matrix of residuals at each iteration (n x B+1)
#'     Column 0 contains initial residuals (y), columns 1:B contain updated residuals
#'   \item predictions_history: Matrix of base learner predictions (n x B)
#'   \item eta: The shrinkage parameter used
#'   \item B: Number of iterations
#' }
#' 
#' @details
#' For squared error loss L(y,f) = 0.5*(y-f)^2, the negative gradient is
#' -dL/df = y - f, which equals the residuals. Each iteration fits a base
#' learner to these residuals (pseudo-residuals) and updates predictions.
#' 
#' @examples
#' \dontrun{
#' library(rpart)
#' 
#' # Model creator function
#' model_creator <- function() {
#'   Model$new(rpart::rpart)
#' }
#' 
#' # Generate data
#' set.seed(123)
#' X <- matrix(rnorm(200), ncol = 4)
#' y <- 2*X[,1] - 1.5*X[,2] + rnorm(50)
#' 
#' # Run boosting
#' result <- boost_regression(
#'   model_creator = model_creator,
#'   X = X, y = y, 
#'   B = 100, eta = 0.1,
#'   model_args = list(maxdepth = 2)
#' )
#' 
#' # Make predictions
#' preds <- predict_boost(result, X)
#' sqrt(mean((y - preds)^2))  # RMSE
#' }
#' 
#' @export
boost_regression <- function(model_creator, X, y, B, model_args, eta = 0.1, verbose = TRUE) {
    .Call(`_unifiedml_boost_regression`, model_creator, X, y, B, model_args, eta, verbose)
}

#' Predict using a Boosted Model
#' 
#' Generate predictions from a boosted model on new data.
#' 
#' @param boost_obj A boosted model object from boost_regression()
#' @param X_new Feature matrix for prediction (m x p)
#' 
#' @return Vector of predictions (length m)
#' 
#' @export
predict_boost <- function(boost_obj, X_new) {
    .Call(`_unifiedml_predict_boost`, boost_obj, X_new)
}

#' Compute Variable Importance for Boosted Model
#' 
#' Calculate relative importance of each feature based on correlation
#' with base learner predictions, weighted by SSE improvement.
#' 
#' @param boost_obj A boosted model object from boost_regression()
#' @param normalize Whether to normalize importances to sum to 100
#' 
#' @return Vector of variable importances (length p)
#' 
#' @details
#' This is a generic, correlation-based importance metric suitable for
#' any base learner. For tree-based models, consider using model-specific
#' importance measures if available. The importance for feature j is
#' computed as sum over iterations of:
#' SSE_improvement * |correlation(X_j, f_b)|
#' 
#' @export
variable_importance_boost <- function(boost_obj, normalize = TRUE) {
    .Call(`_unifiedml_variable_importance_boost`, boost_obj, normalize)
}

#' Compute Variable Importance for Boosted Model (with X)
#' 
#' Calculate relative importance of each feature based on correlation
#' with base learner predictions, weighted by SSE improvement.
#' 
#' @param boost_obj A boosted model object from boost_regression()
#' @param X Training feature matrix (n x p)
#' @param normalize Whether to normalize importances to sum to 100
#' 
#' @return Vector of variable importances (length p)
#' 
#' @details
#' This computes a generic importance metric by:
#' 1. Computing SSE improvement from each iteration
#' 2. Computing correlation between each feature and base learner predictions
#' 3. Allocating improvement proportional to squared correlation
#' 
#' Note: This is a heuristic proxy. For tree-based models, model-specific
#' importance measures (e.g., split improvement) may be more accurate.
#' 
#' @export
variable_importance_boost_with_X <- function(boost_obj, X, normalize = TRUE) {
    .Call(`_unifiedml_variable_importance_boost_with_X`, boost_obj, X, normalize)
}

#' Compute Training Loss History
#' 
#' Calculate MSE on training data at each boosting iteration.
#' 
#' @param boost_obj A boosted model object from boost_regression()
#' 
#' @return Vector of MSE values (length B+1), starting with initial MSE
#' 
#' @export
compute_loss_history <- function(boost_obj) {
    .Call(`_unifiedml_compute_loss_history`, boost_obj)
}

rcpp_hello_world <- function() {
    .Call(`_unifiedml_rcpp_hello_world`)
}

